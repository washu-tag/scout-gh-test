---
- name: Install Temporal and workers
  hosts: server
  gather_facts: false

  environment:
    HELM_PLUGINS: '{{ helm_plugins_dir }}'
    KUBECONFIG: '{{ kubeconfig_yaml }}'
  tasks:
    - name: Add Temporal Helm repository
      kubernetes.core.helm_repository:
        name: temporal
        repo_url: https://go.temporal.io/helm-charts

    - name: Install/Upgrade Temporal Helm chart
      kubernetes.core.helm:
        name: temporal
        chart_ref: temporal/temporal
        chart_version: ^0.55.0
        release_namespace: temporal
        create_namespace: true
        release_state: present
        update_repo_cache: true
        wait: true
        wait_timeout: 10m
        atomic: true
        values:
          prometheus:
            enabled: false
          grafana:
            enabled: false
          server:
            replicaCount: 1
            config:
              namespaces:
                create: true
          cassandra:
            config:
              cluster_size: 1
          elasticsearch:
            replicas: 1
          web:
            additionalEnv:
              - name: TEMPORAL_CSRF_COOKIE_INSECURE
                value: 'true'

    - name: Create namespace
      kubernetes.core.k8s:
        name: '{{ orchestration_worker_namespace }}'
        api_version: v1
        kind: Namespace
        state: present

    #TODO It might be better to set up secret & configmap from helm chart, but then they collide
    # because we install the same chart twice with different values for the two types of workers
    # putting them in different namespaces might solve this issue
    - name: Create S3 secret
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: v1
          kind: Secret
          metadata:
            name: s3-secret
            namespace: '{{ orchestration_worker_namespace }}'
          type: Opaque
          stringData:
            AWS_ACCESS_KEY_ID: '{{ s3_username }}'
            AWS_SECRET_ACCESS_KEY: '{{ s3_password }}'

    - name: Create S3 configmap
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: s3-env
            namespace: '{{ orchestration_worker_namespace }}'
          data:
            AWS_ENDPOINT_URL: 'http://minio.{{ minio_tenant_namespace }}'
            AWS_ALLOW_HTTP: 'true'
            AWS_REGION: '{{ s3_region }}'

    - name: Deploy Java worker
      kubernetes.core.helm:
        name: temporal-java
        chart_ref: '{{ scout_repo_dir }}/helm/orchestration/temporal-worker'
        values_files:
          - '{{ scout_repo_dir }}/helm/orchestration/temporal-java.values.yaml'
        release_namespace: '{{ orchestration_worker_namespace }}'
        release_state: present
        wait: true
        wait_timeout: 5m
        atomic: true
        values:
          config:
            application:
              s3:
                endpoint: 'http://minio.{{ minio_tenant_namespace }}'
                region: '{{ s3_region }}'
          image:
            repository: '{{ temporal_java_image | default(omit) }}'
          volumes:
            - name: hl7logs
              hostPath:
                path: /var/lib/scout/tests/staging_test_data/hl7

    - name: Deploy Python worker
      kubernetes.core.helm:
        name: temporal-python
        chart_ref: '{{ scout_repo_dir }}/helm/orchestration/temporal-worker'
        values_files:
          - '{{ scout_repo_dir }}/helm/orchestration/temporal-python.values.yaml'
        release_namespace: '{{ orchestration_worker_namespace }}'
        release_state: present
        wait: true
        wait_timeout: 5m
        atomic: true
        values:
          image:
            repository: '{{ temporal_python_image | default(omit) }}'
